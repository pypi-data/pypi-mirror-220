%YAML 1.2
---
training_type: "diffuser" # Type of training to run. Either "diffuser" or "transformer".
name: "test-config-file" # Name of the experiment.
api-key: "YOUR API KEY HERE" # Your Cerebrium API key.
hf_model_path: "runwayml/stable-diffusion-v1-5"
revision: "main" # Revision of the diffuser model to use.
train_prompt: "Photo of a tsdf dog."
validation_prompt: ~ # an optional validation prompt to use. If ~, will use the training prompt.
custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.
log_level: "WARNING" # log_level level for logging.

train_image_dir: data/training_class_images/ # Directory of training images.
prior_class_image_dir: ~ # "data/prior_class_images" # Optional directory of images to use for prior class.
prior_class_prompt: "Photo of a dog."

# Training params
training_args:
  # General training params
  num_validation_images: 4 # Number of images to generate in validation.
  learning_rate: 1.0E-4
  num_train_epochs: 30
  seed: ~
  resolution: 512 # Resolution to train images at.
  center_crop: False # Whether to center crop images to resolution.
  train_batch_size: 2
  num_prior_class_images: 10
  prior_class_generation_batch_size: 2
  prior_loss_weight: 1.0 # Weight of prior loss in the total loss.
  max_train_steps: ~ # maximum training steps which overrides number of training epochs
  validation_epochs: 10 # number of epochs before running validation and checkpointing

  # Training loop params
  gradient_accumulation_steps: 1
  lr_scheduler: "constant"
  lr_warmup_steps: 5
  lr_num_cycles: 1
  lr_power: 1.0
  allow_tf32: False
  max_grad_norm: 1.0
  mixed_precision: "no" # "fp16 or "bf16"
  prior_generation_precision: ~
  scale_lr: False
  use_8bit_adam: True
  use_xformers: True # Whether to use xformers memory efficient attention or not.
