# -*- coding: utf-8 -*-
# @Time    : 2022/7/14 4:46 PM
# @Author  : lch
# @Email   : iltie165@163.com
# @File    : dist_ezoo_replace.py.py
from dgl.distributed import rpc
from dgl.distributed.kvstore import KVServer, init_kvstore
from dgl.distributed.dist_graph import _copy_graph_to_shared_mem
from dgl.distributed.graph_partition_book import HeteroDataName, PartitionPolicy, RangePartitionBook
from dgl.distributed.server_state import ServerState
from dgl.distributed.rpc_server import start_server
from dgl.distributed.rpc_client import connect_to_server
from dgl.distributed.dist_context import CustomPool
from dgl.distributed.role import init_role
import sys
import json
import numpy as np
import os
import dgl
import torch as th

MAX_QUEUE_SIZE = 20*1024*1024*1024


def dist_ezoo_initialize(ip_config, e_graph, part_config, submit_type, num_servers=1, num_clients=1,
                         max_queue_size=MAX_QUEUE_SIZE, net_type='tensorpipe',
                         num_worker_threads=1):
    if os.environ.get('DGL_ROLE', 'client') == 'server':
        assert os.environ.get('DGL_SERVER_ID') is not None, \
            'Please define DGL_SERVER_ID to run DistGraph server'
        formats = os.environ.get('DGL_GRAPH_FORMAT', 'csc').split(',')
        formats = [f.strip() for f in formats]
        rpc.reset()
        keep_alive = bool(int(os.environ.get('DGL_KEEP_ALIVE', 0)))
        serv = EzooDistGraphServer(int(os.environ.get('DGL_SERVER_ID')),
                               ip_config,
                               int(os.environ.get('DGL_NUM_SERVER', num_servers)),
                               int(os.environ.get('DGL_NUM_CLIENT', num_clients)),
                               part_config,
                               e_graph,
                               submit_type,
                               graph_format=formats,
                               keep_alive=keep_alive,
                               net_type=net_type)
        serv.start()
        sys.exit()
    else:
        if os.environ.get('DGL_NUM_SAMPLER') is not None:
            num_workers = int(os.environ.get('DGL_NUM_SAMPLER'))
        else:
            num_workers = 0
        if os.environ.get('DGL_NUM_SERVER') is not None:
            num_servers = int(os.environ.get('DGL_NUM_SERVER'))
        else:
            num_servers = 1
        group_id = int(os.environ.get('DGL_GROUP_ID', 0))
        rpc.reset()
        global SAMPLER_POOL
        global NUM_SAMPLER_WORKERS
        is_standalone = os.environ.get(
            'DGL_DIST_MODE', 'standalone') == 'standalone'
        if num_workers > 0 and not is_standalone:
            SAMPLER_POOL = CustomPool(num_workers, (ip_config, num_servers, max_queue_size,
                                                    net_type, 'sampler', num_worker_threads,
                                                    group_id))
        else:
            SAMPLER_POOL = None
        NUM_SAMPLER_WORKERS = num_workers
        if not is_standalone:
            assert num_servers is not None and num_servers > 0, \
                'The number of servers per machine must be specified with a positive number.'
            connect_to_server(ip_config, num_servers, max_queue_size, net_type, group_id=group_id)
        init_role('default')
        init_kvstore(ip_config, num_servers, 'default')


class EzooDistGraphServer(KVServer):
    ''' The DistGraph server.

    This DistGraph server loads the graph data and sets up a service so that trainers and
    samplers can read data of a graph partition (graph structure, node data and edge data)
    from remote machines. A server is responsible for one graph partition.

    Currently, each machine runs only one main server with a set of backup servers to handle
    clients' requests. The main server and the backup servers all handle the requests for the same
    graph partition. They all share the partition data (graph structure and node/edge data) with
    shared memory.

    By default, the partition data is shared with the DistGraph clients that run on
    the same machine. However, a user can disable shared memory option. This is useful for the case
    that a user wants to run the server and the client on different machines.

    Parameters
    ----------
    server_id : int
        The server ID (start from 0).
    ip_config : str
        Path of IP configuration file.
    num_servers : int
        Server count on each machine.
    num_clients : int
        Total number of client nodes.
    part_config : string
        The path of the config file generated by the partition tool.
    disable_shared_mem : bool
        Disable shared memory.
    graph_format : str or list of str
        The graph formats.
    keep_alive : bool
        Whether to keep server alive when clients exit
    net_type : str
        Backend rpc type: ``'socket'`` or ``'tensorpipe'``
    '''

    def __init__(self, server_id, ip_config, num_servers,
                 num_clients, ezoo_part_config, e_graph,
                 submit_type, num_halo = 1, disable_shared_mem=False,
                 graph_format=('csc', 'coo'),
                 keep_alive=False, net_type='tensorpipe'):
        super(EzooDistGraphServer, self).__init__(server_id=server_id,
                                                  ip_config=ip_config,
                                                  num_servers=num_servers,
                                                  num_clients=num_clients)
        self.e_graph = e_graph
        self.ip_config = ip_config
        self.num_servers = num_servers
        self.keep_alive = keep_alive
        self.net_type = net_type
        # TODO 后面再支持back_server
        # Load graph partition data.
        # if self.is_backup_server():
        #     # The backup server doesn't load the graph partition. It'll initialized afterwards.
        #     self.gpb, graph_name, ntypes, etypes = load_partition_book(part_config, self.part_id, )
        #     self.client_g = None
        # else:
        combine_nodes_feats = combine_edges_feats = graph_name = ntypes = etypes = None

        if (submit_type == 0):
            self.client_g, combine_nodes_feats, combine_edges_feats, self.gpb, graph_name, \
                ntypes, etypes = self.load_partition_from_memory(ezoo_part_config)
        else:
            with open(ezoo_part_config) as conf_f:
                part_metadata = json.load(conf_f)
            self.check_config(part_metadata)
            config_path = os.path.dirname(ezoo_part_config)
            relative_to_config = lambda path: os.path.join(config_path, path)

            self.client_g, combine_nodes_feats, combine_edges_feats, self.gpb, graph_name, \
                ntypes, etypes = self.load_partition_from_ezoo(part_metadata, relative_to_config, num_halo)

        print('load ' + graph_name)
        # Create the graph formats specified the users.
        self.client_g = self.client_g.formats(graph_format)
        self.client_g.create_formats_()
        if not disable_shared_mem:
            self.client_g = _copy_graph_to_shared_mem(self.client_g, graph_name, graph_format)

        if not disable_shared_mem:
            self.gpb.shared_memory(graph_name)
        assert self.gpb.partid == self.part_id
        for ntype in ntypes:
            node_name = HeteroDataName(True, ntype, None)
            self.add_part_policy(PartitionPolicy(node_name.policy_str, self.gpb))
        for etype in etypes:
            edge_name = HeteroDataName(False, etype, None)
            self.add_part_policy(PartitionPolicy(edge_name.policy_str, self.gpb))

        if not self.is_backup_server():
            for name in combine_nodes_feats:
                # The feature name has the following format: node_type + "/" + feature_name to avoid
                # feature name collision for different node types.
                ntype, feat_name = name.split('/')
                data_name = HeteroDataName(True, ntype, feat_name)
                self.init_data(name=str(data_name), policy_str=data_name.policy_str,
                               data_tensor=combine_nodes_feats[name])
                self.orig_data.add(str(data_name))

            # TODO 后面再处理边特征
            # for name in edge_feats:
            #     # The feature name has the following format: edge_type + "/" + feature_name to avoid
            #     # feature name collision for different edge types.
            #     etype, feat_name = name.split('/')
            #     data_name = HeteroDataName(False, etype, feat_name)
            #     self.init_data(name=str(data_name), policy_str=data_name.policy_str,
            #                    data_tensor=edge_feats[name])
            #     self.orig_data.add(str(data_name))

    
    def split_train_val_test(self, num_nodes):
        train_mask = np.zeros(num_nodes)   
        val_mask = np.zeros(num_nodes)
        test_mask = np.zeros(num_nodes)

        tvt_split_idx = np.random.permutation(num_nodes)
        dataset_rate = [0.6, 0.8, 1.0]
        train_idx = tvt_split_idx[: int(num_nodes * dataset_rate[0])]
        val_idx = tvt_split_idx[int(num_nodes * dataset_rate[0]) : int(num_nodes * dataset_rate[1])]
        test_idx = tvt_split_idx[int(num_nodes * dataset_rate[1]) :]
        train_mask[train_idx] = 1
        val_mask[val_idx] = 1
        test_mask[test_idx] = 1

        return th.from_numpy(train_mask.astype(np.int32)), \
                th.from_numpy(val_mask.astype(np.int32)),  \
                th.from_numpy(test_mask.astype(np.int32))

    def check_config(self, part_metadata):
        assert 'num_parts' in part_metadata, 'num_parts does not exist.'
        assert part_metadata['num_parts'] > self.part_id, \
            'part {} is out of range (#parts: {})'.format(self.part_id, part_metadata['num_parts'])
        assert 'node_map' in part_metadata, "cannot get the node map."
        assert 'edge_map' in part_metadata, "cannot get the edge map."
        assert 'graph_name' in part_metadata, "cannot get the graph name"


    def load_partition_book(self, node_map, edge_map, num_parts, ntypes, etypes, subgraph):
        assert len(node_map.shape) != 2, "only support range book."
        return RangePartitionBook(self.part_id, num_parts,
                            {'_N': node_map.astype(np.long)},
                            {'_E': edge_map.astype(np.long)},
                            ntypes, etypes)
        

    def load_partition_from_memory(self, part_data):
        num_parts = part_data['num_parts']

        src = part_data['sub_graph_u']
        dst = part_data['sub_graph_v']

        orig_nodes = part_data['orig_node']
        orig_edges = part_data['orig_edge']

        true_orig_nodes = part_data['true_orig_node']

        num_inner = part_data['num_inner']
        inner_nodes = np.zeros(true_orig_nodes.size)
        inner_nodes[:num_inner] = 1
        inner_edges = np.ones(src.size)

        subgraph = dgl.graph((src.astype(np.long), dst.astype(np.long)), idtype=th.int64)
        subgraph.ndata['inner_node'] = th.from_numpy(inner_nodes.astype(np.long))
        subgraph.ndata['_ID'] = th.from_numpy(orig_nodes.astype(np.long))
        subgraph.edata['_ID'] = th.from_numpy(orig_edges.astype(np.long))
        subgraph.ndata['true_orig_id'] = th.from_numpy(true_orig_nodes.astype(np.long))
        subgraph.edata['inner_edge'] = th.from_numpy(inner_edges.astype(np.long))

        ntypes = {'_N': 0}
        etypes = {'_E': 0}
        graph_name = part_data['graph_name']
        gpb = self.load_partition_book(part_data['node_map'], 
                                       part_data['edge_map'],
                                       num_parts,
                                       ntypes,
                                       etypes,
                                       subgraph)

        combine_nodes_feats = self.load_features(true_orig_nodes[:num_inner], num_inner)
        return subgraph, combine_nodes_feats, None, gpb, graph_name, ntypes, etypes

    def load_partition_from_ezoo(self, part_metadata, path_wrapper, num_halo):
        num_parts = part_metadata['num_parts']

        split_nodes = np.load(path_wrapper(part_metadata['split_nodes']))
        src, dst, inner_nodes, orig_nodes, \
        inner_edges, orig_edges, sub_node_num, \
        sub_edge_num = self.e_graph.get_subgraph(split_nodes,
                                                   self.part_id,
                                                   num_parts,
                                                   num_halo)

        subgraph = dgl.graph((src.astype(np.long), dst.astype(np.long)), idtype=th.int64)

        subgraph.ndata['inner_node'] = th.from_numpy(inner_nodes.astype(np.long))
        subgraph.ndata['_ID'] = th.from_numpy(orig_nodes.astype(np.long))
        #subgraph.edata['inner_edge'] = th.from_numpy(inner_edges)
        subgraph.edata['_ID'] = th.from_numpy(orig_edges.astype(np.long))

        node_map = np.load(path_wrapper(part_metadata['node_map']))
        edge_map = np.load(path_wrapper(part_metadata['edge_map']))
        ntypes = {'_N': 0}
        etypes = {'_E': 0}
        graph_name = part_metadata['graph_name']

        gpb = self.load_partition_book(node_map, 
                                       edge_map,
                                       num_parts,
                                       ntypes,
                                       etypes,
                                       subgraph)

        combine_nodes_feats = self.load_features(orig_nodes[:sub_node_num], sub_node_num)

        return subgraph, combine_nodes_feats, None, gpb, graph_name, ntypes, etypes


    def load_features(self, ids, inner_num, type=''):
        #load feature from ezoo only in partition
        print("ready for load featurs: ")
        label_and_features = self.e_graph.get_node_props2float('', ids)

        # Get labels and features(get rid of label and id columns).
        label_index = self.e_graph.get_node_prop_index_from_name(
            'node', 'label')
        labels = label_and_features[:, label_index]
        id_index = self.e_graph.get_node_prop_index_from_name(
            'node', 'id')

        # filter useless properties
        ex_index_list = []
        ex_index_list.extend([id_index, label_index])

        features = np.delete(label_and_features, ex_index_list, axis=1)
        max_feature_num = features.max()

        train_mask_th, val_mask_th, test_mask_th = self.split_train_val_test(inner_num)

        #_N/feat, _N/label
        combine_nodes_feats = {'_N/feat': th.from_numpy(features), 
                         '_N/label': th.from_numpy(labels.astype(np.long)),
                         '_N/train_mask': train_mask_th,
                         '_N/val_mask': val_mask_th,
                         '_N/test_mask': test_mask_th}

        print("load features finished.")
        return combine_nodes_feats

    def start(self):
        """ Start graph store server.
        """
        # start server
        server_state = ServerState(kv_store=self, local_g=self.client_g,
                                   partition_book=self.gpb, keep_alive=self.keep_alive)
        print('start graph service on server {} for part {}'.format(
            self.server_id, self.part_id))
        start_server(server_id=self.server_id,
                     ip_config=self.ip_config,
                     num_servers=self.num_servers,
                     num_clients=self.num_clients,
                     server_state=server_state,
                     net_type=self.net_type)
