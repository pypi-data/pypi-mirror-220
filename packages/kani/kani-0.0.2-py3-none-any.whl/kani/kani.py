import asyncio
import inspect
import logging
from typing import AsyncIterable, Callable

import cachetools

from .ai_function import AIFunction
from .engines.base import BaseEngine
from .exceptions import NoSuchFunction, WrappedCallException, FunctionCallException
from .models import ChatMessage, FunctionCall, ChatRole
from .utils.typing import PathLike, SavedKani

log = logging.getLogger(__name__)


class Kani:
    """Base class for all kani.

    **Entrypoints**

    ``chat_round(query: str, **kwargs) -> ChatMessage``

    ``full_round(query: str, function_call_formatter: Callable[[ChatMessage], str], **kwargs) -> AsyncIterable[ChatMessage]``

    ``chat_round_str(query: str, **kwargs) -> str``

    ``full_round_str(query: str, function_call_formatter: Callable[[ChatMessage], str], **kwargs) -> AsyncIterable[str]``

    **Function Calling**

    Subclass and use ``@ai_function()`` to register functions. The schema will be autogenerated from the function
    signature (see :func:`ai_function`).

    To perform a chat round with functions, use :meth:`full_round()` as an async iterator::

        async for msg in kani.full_round(prompt):
            # responses...

    Each response will be a :class:`.ChatMessage`.

    Alternatively, you can use :meth:`full_round_str` and control the format of a yielded function call with
    ``function_call_formatter``.

    **Retry & Model Feedback**

    If the model makes an error when attempting to call a function (e.g. calling a function that does not exist or
    passing params with incorrect and non-coercible types) or the function raises an exception, Kani will send the
    error in a system message to the model, allowing it up to *retry_attempts* to correct itself and retry the call.
    """

    def __init__(
        self,
        engine: BaseEngine,
        system_prompt: str = None,
        always_include_messages: list[ChatMessage] = None,
        desired_response_tokens: int = 450,
        chat_history: list[ChatMessage] = None,
        functions: list[AIFunction] = None,
        retry_attempts: int = 1,
    ):
        """
        :param engine: The LM engine implementation to use.
        :param system_prompt: The system prompt to provide to the LM. The prompt will not be included in
            :attr:`chat_history`.
        :param always_include_messages: A list of messages to always include as a prefix in all chat rounds (i.e.,
            evict newer messages rather than these to manage context length). These will not be included in
            :attr:`chat_history`.
        :param desired_response_tokens: The minimum amount of space to leave in ``max context size - tokens in prompt``.
            To control the maximum number of tokens generated more precisely, you may be able to configure the engine
            (e.g. ``OpenAIEngine(..., max_tokens=250)``).
        :param chat_history: The chat history to start with (not including system prompt or always include messages),
            for advanced use cases. By default, each kani starts with a new conversation session.
        :param functions: A list of :class:`.AIFunction` to expose to the model (for dynamic function calling).
            Use :func:`.ai_function` to define static functions (see :doc:`function_calling`).
        :param retry_attempts: How many attempts the LM may take if a function call raises an exception.
        """
        self.engine = engine
        self.system_prompt = system_prompt.strip() if system_prompt else None
        self.desired_response_tokens = desired_response_tokens
        self.max_context_size = engine.max_context_size

        self.always_include_messages: list[ChatMessage] = (
            [ChatMessage.system(self.system_prompt)] if system_prompt else []
        ) + (always_include_messages or [])
        """Chat messages that are always included as a prefix in the model's prompt. Includes the system message, if
        supplied."""

        self.chat_history: list[ChatMessage] = chat_history or []
        """All messages in the current chat state, not including system or always include messages."""

        # async to prevent generating multiple responses missing context
        self.lock = asyncio.Lock()

        # function calling
        self.retry_attempts = retry_attempts

        # find all registered ai_functions and save them
        if functions is None:
            functions = []
        self.functions = {f.name: f for f in functions}
        for name, member in inspect.getmembers(self, predicate=inspect.ismethod):
            if not hasattr(member, "__ai_function__"):
                continue
            f: AIFunction = AIFunction(member, **member.__ai_function__)
            if f.name in self.functions:
                raise ValueError(f"AIFunction {f.name!r} is already registered!")
            self.functions[f.name] = f

        # cache
        self._oldest_idx = 0
        self._message_tokens = cachetools.FIFOCache(256)

    def message_token_len(self, message: ChatMessage):
        """Returns the number of tokens used by a given message."""
        try:
            return self._message_tokens[message]
        except KeyError:
            mlen = self.engine.message_len(message)
            if mlen > self.max_context_size:
                log.warning(
                    "The chat message's size is longer than the entire context window. It will never be included in a"
                    f" prompt, nor any messages before it.\nContent: {message.content!r}"
                )
            self._message_tokens[message] = mlen
            return mlen

    # === main entrypoints ===
    async def chat_round(self, query: str, **kwargs) -> ChatMessage:
        """Perform a single chat round (user -> model -> user, no functions allowed).

        This is slightly faster when you are chatting with a kani with no AI functions defined.

        :param query: The contents of the user's chat message.
        :param kwargs: Additional arguments to pass to the model engine (e.g. hyperparameters).
        :returns: The model's reply.
        """
        async with self.lock:
            # get the user's chat input
            self.chat_history.append(ChatMessage.user(query.strip()))

            # get the context
            messages = await self.get_truncated_chat_history()

            # get the model's output, save it to chat history
            completion = await self.engine.predict(messages=messages, **kwargs)

            message = completion.message
            self._message_tokens[message] = completion.completion_tokens or self.message_token_len(message)
            self.chat_history.append(message)
            return message

    async def chat_round_str(self, query: str, **kwargs) -> str:
        """Like :meth:`chat_round`, but only returns the content of the message."""
        msg = await self.chat_round(query, **kwargs)
        return msg.content

    async def full_round(self, query: str, **kwargs) -> AsyncIterable[ChatMessage]:
        """Perform a full chat round (user -> model [-> function -> model -> ...] -> user).

        Yields each of the model's ChatMessages. A ChatMessage must have at least one of (content, function_call).

        Use this in an async for loop, like so::

            async for msg in kani.full_round("How's the weather?"):
                print(msg.content)

        :param query: The content of the user's chat message.
        :param kwargs: Additional arguments to pass to the model engine (e.g. hyperparameters).
        """
        retry = 0
        is_model_turn = True
        async with self.lock:
            self.chat_history.append(ChatMessage.user(query.strip()))

            while is_model_turn:
                # do the model prediction
                messages = await self.get_truncated_chat_history()
                completion = await self.engine.predict(
                    messages=messages, functions=list(self.functions.values()), **kwargs
                )
                # bookkeeping
                message = completion.message
                self._message_tokens[message] = completion.completion_tokens or self.message_token_len(message)
                self.chat_history.append(message)
                yield message

                # if function call, do it and attempt retry if it's wrong
                if not message.function_call:
                    return

                try:
                    is_model_turn = await self.do_function_call(message.function_call)
                except FunctionCallException as e:
                    should_retry = await self.handle_function_call_exception(message.function_call, e, retry)
                    # retry if we have retry attempts left
                    retry += 1
                    if not should_retry:
                        # disable function calling on the next go
                        kwargs = {**kwargs, "function_call": "none"}
                    continue
                else:
                    retry = 0

    async def full_round_str(
        self,
        query: str,
        function_call_formatter: Callable[[ChatMessage], str | None] = lambda _: None,
        **kwargs,
    ) -> AsyncIterable[str]:
        """Like :meth:`full_round`, but each yielded element is a str rather than a ChatMessage.

        :param query: The content of the user's chat message.
        :param function_call_formatter: A function that returns a string to yield when the model decides to call a
            function (or None to yield nothing). By default, ``full_round_str`` does not yield on a function call.
        :param kwargs: Additional arguments to pass to the model engine (e.g. hyperparameters).
        """
        async for message in self.full_round(query, **kwargs):
            if text := message.content:
                yield text

            if message.function_call and (fn_msg := function_call_formatter(message)):
                yield fn_msg

    # ==== overridable methods ====
    async def get_truncated_chat_history(self) -> list[ChatMessage]:
        """
        Called each time before asking the LM engine for a completion to generate the chat prompt.
        Returns a list of messages such that the total token count in the messages is less than
        ``(self.max_context_size - self.desired_response_tokens)``.

        Always includes the system prompt plus any always_include_messages at the start of the prompt.

        You may override this to get more fine-grained control over what is exposed in the model's memory at any given
        call.
        """
        reversed_history = []
        always_len = sum(self.message_token_len(m) for m in self.always_include_messages) + self.engine.token_reserve
        remaining = self.max_context_size - (always_len + self.desired_response_tokens)
        total_tokens = 0
        for idx in range(len(self.chat_history) - 1, self._oldest_idx - 1, -1):
            message = self.chat_history[idx]
            message_len = self.message_token_len(message)
            remaining -= message_len
            if remaining > 0:
                total_tokens += message_len
                reversed_history.append(message)
            else:
                self._oldest_idx = idx + 1
                break
        log.debug(
            f"get_truncated_chat_history() returned {always_len + total_tokens} tokens ({always_len} always) in"
            f" {len(self.always_include_messages) + len(reversed_history)} messages"
            f" ({len(self.always_include_messages)} always)"
        )
        return self.always_include_messages + reversed_history[::-1]

    async def do_function_call(self, call: FunctionCall) -> bool:
        """Resolve a single function call.

        By default, any exception raised from this method will be an instance of a :class:`.FunctionCallException`.

        You may implement an override to add instrumentation around function calls (e.g. tracking success counts
        for varying prompts). See :ref:`do_function_call`.

        :returns: True (default) if the model should immediately react; False if the user speaks next.
        :raises NoSuchFunction: The requested function does not exist.
        :raises WrappedCallException: The function raised an exception.
        """
        log.debug(f"Model requested call to {call.name} with data: {call.arguments!r}")
        # get func
        f = self.functions.get(call.name)
        if not f:
            raise NoSuchFunction(call.name)
        # call it
        try:
            result = await f(**call.kwargs)
            result_str = str(result)
        except Exception as e:
            raise WrappedCallException(f.auto_retry, e) from e
        # save the result to the chat history
        log.debug(f"{f.name} responded with data: {result_str!r}")
        self.chat_history.append(ChatMessage.function(f.name, result_str))
        # yield whose turn it is
        return f.after == ChatRole.ASSISTANT

    async def handle_function_call_exception(
        self, call: FunctionCall, err: FunctionCallException, attempt: int
    ) -> bool:
        """Called when a function call raises an exception.

        By default, this adds a message to the chat telling the LM about the error and allows a retry if the error
        is recoverable and there are remaining retry attempts.

        You may implement an override to customize the error prompt, log the error, or use custom retry logic.
        See :ref:`handle_function_call_exception`.

        :param call: The :class:`.FunctionCall` the model was attempting to make.
        :param err: The error the call raised. Usually this is :class:`.NoSuchFunction` or
            :class:`.WrappedCallException`, although it may be any exception raised by :meth:`do_function_call`.
        :param attempt: The attempt number for the current call (0-indexed).
        :returns: True if the model should retry the call; False if not.
        """
        # log the exception here
        log.debug(f"Call to {call.name} raised an exception: {err}")
        # tell the model what went wrong
        if isinstance(err, NoSuchFunction):
            self.chat_history.append(
                ChatMessage.system(f"The function {err.name!r} is not defined. Only use the provided functions.")
            )
        else:
            # but if it's a user function error, we want to raise it
            log.error(f"Call to {call.name} raised an exception: {err}", exc_info=err)
            self.chat_history.append(ChatMessage.function(call.name, str(err)))

        return attempt < self.retry_attempts and err.retry

    # ==== utility methods ====
    def save(self, fp: PathLike, **kwargs):
        """Save the chat state of this kani to a JSON file. This will overwrite the file if it exists!

        :param fp: The path to the file to save.
        :param kwargs: Additional arguments to pass to Pydantic's ``model_dump_json``.
        """
        data = SavedKani(always_include_messages=self.always_include_messages, chat_history=self.chat_history)
        with open(fp, "w") as f:
            f.write(data.model_dump_json(**kwargs))

    def load(self, fp: PathLike, **kwargs):
        """Load chat state from a JSON file into this kani. This will overwrite any existing chat state!

        :param fp: The path to the file containing the chat state.
        :param kwargs: Additional arguments to pass to Pydantic's ``model_validate_json``.
        """
        with open(fp) as f:
            data = f.read()
        state = SavedKani.model_validate_json(data, **kwargs)
        self.always_include_messages = state.always_include_messages
        self.chat_history = state.chat_history
