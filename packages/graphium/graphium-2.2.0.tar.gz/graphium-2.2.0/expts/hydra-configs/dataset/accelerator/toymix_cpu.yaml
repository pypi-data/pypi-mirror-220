# @package _global_

architecture:
  task_heads:
    tox21:
      last_activation: none

datamodule:
  args:
    batch_size_training: 200
    batch_size_inference: 200
    featurization_n_jobs: 4
    num_workers: 4

predictor:
  optim_kwargs: {}
  loss_fun:
    tox21: bce_logits_ipu
  metrics_every_n_steps: 300
  torch_scheduler_kwargs:
    max_num_epochs: &max_epochs 300

trainer:
  trainer:
    precision: 32
    accumulate_grad_batches: 1
    max_epochs: *max_epochs