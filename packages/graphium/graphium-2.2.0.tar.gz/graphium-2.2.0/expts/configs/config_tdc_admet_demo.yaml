# Testing the gcn model with the PCQMv2 dataset on IPU.
constants:
  name: &name tdc_admet_demo
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training

accelerator:
  type: gpu  # cpu or ipu or gpu

datamodule:
  module_type: "ADMETBenchmarkDataModule"
  args:
    # TDC specific
    tdc_benchmark_names: null
    tdc_train_val_seed: *seed
    # Featurization
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: 30
    featurization_progress: True
    featurization_backend: "loky"
    processed_graph_data_path: "../datacache/tdc-admet-demo/"
    featurization:
      atom_property_list_onehot: [atomic-number, group, period, total-valence]
      atom_property_list_float: [degree, formal-charge, radical-electron, aromatic, in-ring]
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False # if H is included
      use_bonds_weights: False
      pos_encoding_as_features:
        pos_types:
          lap_eigvec:
            pos_level: node
            pos_type: laplacian_eigvec
            num_pos: 8
            normalization: "none" # nomrlization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          lap_eigval:
            pos_level: node
            pos_type: laplacian_eigval
            num_pos: 8
            normalization: "none" # nomrlization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          rw_pos: # use same name as pe_encoder
            pos_level: node
            pos_type: rw_return_probs
            ksteps: 16

    num_workers: -1 # -1 to use all
    persistent_workers: False # if use persistent worker at the start of each epoch.


architecture:
  model_type: FullGraphMultiTaskNetwork
  mup_base_path: null
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: 64
    hidden_dims: 256
    depth: 2
    activation: relu
    last_activation: none
    dropout: &dropout 0.18
    normalization: &normalization layer_norm
    last_normalization: *normalization
    residual_type: none

  pre_nn_edges: null   # Set as null to avoid a pre-nn network

  pe_encoders:
    out_dim: 32
    pool: "sum" #"mean" "max"
    last_norm: None #"batch_norm", "layer_norm"
    encoders: #la_pos |  rw_pos
      la_pos:  # Set as null to avoid a pre-nn network
        encoder_type: "laplacian_pe"
        input_keys: ["laplacian_eigvec", "laplacian_eigval"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        model_type: 'DeepSet' #'Transformer' or 'DeepSet'
        num_layers: 2
        num_layers_post: 1 # Num. layers to apply after pooling
        dropout: 0.1
        first_normalization: "none" #"batch_norm" or "layer_norm"
      rw_pos:
        encoder_type: "mlp"
        input_keys: ["rw_return_probs"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        num_layers: 2
        dropout: 0.1
        normalization: "layer_norm" #"batch_norm" or "layer_norm"
        first_normalization: "layer_norm" #"batch_norm" or "layer_norm"



  gnn:  # Set as null to avoid a post-nn network
    in_dim: 64 # or otherwise the correct value
    out_dim: &gnn_dim 96
    hidden_dims: *gnn_dim
    depth: 4
    activation: gelu
    last_activation: none
    dropout: 0.1
    normalization: "layer_norm"
    last_normalization: *normalization
    residual_type: simple
    virtual_node: 'none'
    layer_type: 'pyg:gcn' #pyg:gine #'pyg:gps' # pyg:gated-gcn, pyg:gine,pyg:gps
    layer_kwargs: null # Parameters for the model itself. You could define dropout_attn: 0.1


  graph_output_nn:
    graph:
      pooling: [sum]
      out_dim: *gnn_dim
      hidden_dims: *gnn_dim
      depth: 1
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

  task_heads:
    caco2_wang: &regression_head
      task_level: graph
      out_dim: 1
      hidden_dims: 64
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
    hia_hou: &classification_head
      task_level: graph
      out_dim: 1
      hidden_dims: 64
      depth: 2
      activation: relu
      last_activation: sigmoid
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
    pgp_broccatelli: *classification_head
    bioavailability_ma: *classification_head
    lipophilicity_astrazeneca: *regression_head
    solubility_aqsoldb: *regression_head
    bbb_martins: *classification_head
    ppbr_az: *regression_head
    vdss_lombardo: *regression_head
    cyp2d6_veith: *classification_head
    cyp3a4_veith: *classification_head
    cyp2c9_veith: *classification_head
    cyp2d6_substrate_carbonmangels: *classification_head
    cyp3a4_substrate_carbonmangels: *classification_head
    cyp2c9_substrate_carbonmangels: *classification_head
    half_life_obach: *regression_head
    clearance_microsome_az: *regression_head
    clearance_hepatocyte_az: *regression_head
    herg: *classification_head
    ames: *classification_head
    dili: *classification_head
    ld50_zhu: *regression_head

#Task-specific
predictor:
  metrics_on_progress_bar:
    # All below metrics are directly copied from the TDC website.
    # For more information, see https://tdcommons.ai/benchmark/admet_group/overview/
    caco2_wang: ["mae"]
    hia_hou: ["auroc"]
    pgp_broccatelli: ["auroc"]
    bioavailability_ma: ["auroc"]
    lipophilicity_astrazeneca: ["mae"]
    solubility_aqsoldb: ["mae"]
    bbb_martins: ["auroc"]
    ppbr_az: ["mae"]
    vdss_lombardo: ["spearman"]
    cyp2d6_veith: ["auprc"]
    cyp3a4_veith: ["auprc"]
    cyp2c9_veith: ["auprc"]
    cyp2d6_substrate_carbonmangels: ["auprc"]
    cyp3a4_substrate_carbonmangels: ["auprc"]
    cyp2c9_substrate_carbonmangels: ["auprc"]
    half_life_obach: ["spearman"]
    clearance_microsome_az: ["spearman"]
    clearance_hepatocyte_az: ["spearman"]
    herg: ["mae"]
    ames: ["auroc"]
    dili: ["auroc"]
    ld50_zhu: ["auroc"]
  loss_fun:
    caco2_wang: mae
    hia_hou: bce
    pgp_broccatelli: bce
    bioavailability_ma: bce
    lipophilicity_astrazeneca: mae
    solubility_aqsoldb: mae
    bbb_martins: bce
    ppbr_az: mae
    vdss_lombardo: mae
    cyp2d6_veith: bce
    cyp3a4_veith: bce
    cyp2c9_veith: bce
    cyp2d6_substrate_carbonmangels: bce
    cyp3a4_substrate_carbonmangels: bce
    cyp2c9_substrate_carbonmangels: bce
    half_life_obach: mae
    clearance_microsome_az: mae
    clearance_hepatocyte_az: mae
    herg: bce
    ames: bce
    dili: bce
    ld50_zhu: mae
  random_seed: *seed
  optim_kwargs:
    lr: 4.e-5 # warmup can be scheduled using torch_scheduler_kwargs
  torch_scheduler_kwargs:
    module_type: WarmUpLinearLR
    max_num_epochs: &max_epochs 10
    warmup_epochs: 10
    verbose: False
  target_nan_mask: null # null: no mask, 0: 0 mask, ignore-flatten, ignore-mean-per-label
  multitask_handling: flatten # flatten, mean-per-label

# Task-specific
metrics:
  caco2_wang: &regression_metrics
    - name: mae
      metric: mae
      target_nan_mask: null
      multitask_handling: flatten
      threshold_kwargs: null
    - name: spearman
      metric: spearmanr
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label
    - name: pearson
      metric: pearsonr
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label
    - name: r2_score
      metric: r2
      target_nan_mask: null
      multitask_handling: mean-per-label
      threshold_kwargs: null
  hia_hou: &classification_metrics
    - name: auroc
      metric: auroc
      task: binary
      multitask_handling: mean-per-label
      threshold_kwargs: null
    - name: auprc
      metric: average_precision
      task: binary
      multitask_handling: mean-per-label
      threshold_kwargs: null
    - name: accuracy
      metric: accuracy
      multitask_handling: mean-per-label
      target_to_int: True
      average: micro
      threshold_kwargs: &threshold_05
        operator: greater
        threshold: 0.5
        th_on_preds: True
        th_on_target: True
    - name: mcc
      metric: mcc
      num_classes: 2
      multitask_handling: mean-per-label
      target_to_int: True
      average: micro
      threshold_kwargs: *threshold_05
  pgp_broccatelli: *classification_metrics
  bioavailability_ma: *classification_metrics
  lipophilicity_astrazeneca: *regression_metrics
  solubility_aqsoldb: *regression_metrics
  bbb_martins: *classification_metrics
  ppbr_az: *regression_metrics
  vdss_lombardo: *regression_metrics
  cyp2d6_veith: *classification_metrics
  cyp3a4_veith: *classification_metrics
  cyp2c9_veith: *classification_metrics
  cyp2d6_substrate_carbonmangels: *classification_metrics
  cyp3a4_substrate_carbonmangels: *classification_metrics
  cyp2c9_substrate_carbonmangels: *classification_metrics
  half_life_obach: *regression_metrics
  clearance_microsome_az: *regression_metrics
  clearance_hepatocyte_az: *regression_metrics
  herg: *classification_metrics
  ames: *classification_metrics
  dili: *classification_metrics
  ld50_zhu: *regression_metrics
trainer:
  seed: *seed
  logger:
    save_dir: logs/tdc-admet-demo/
    name: *name
    project: *name
  model_checkpoint:
    dirpath: models_checkpoints/tdc-admet-demo/
    filename: *name
    save_last: True
  trainer:
    max_epochs: *max_epochs
    min_epochs: 1
    check_val_every_n_epoch: 20
