Metadata-Version: 2.1
Name: llamaapi
Version: 0.1.4
Summary: Llama API python SDK
License: MIT
Author: Eduardo Reis
Author-email: edu.pontes@gmail.com
Requires-Python: >=3.9,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: aiohttp (>=3.8.5,<4.0.0)
Requires-Dist: nest-asyncio (>=1.5.6,<2.0.0)
Requires-Dist: requests (>=2.27.1,<3.0.0)
Description-Content-Type: text/markdown

# LlamaAPI SDK

LlamaAPI is a Python SDK for interacting with the Llama API. It abstracts away the handling of aiohttp sessions and headers, allowing for a simplified interaction with the API.

## Installation
You can install the LlamaAPI SDK using pip:

```python
pip install llamaapi
```

## Usage
After installing the SDK, you can use it in your Python projects like so:

```python
from llamaapi import llamaapi

# Initialize the llamaapi with your api_token
llama = LlamaAPI('<your_api_token>')

# Define your API request
api_request_json = {
  "messages": [
    {"role": "user", "content": "What is the weather like in Boston?"},
  ],
  "functions": [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
          "type": "object",
          "properties": {
              
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "days": {
              "type": "number",
              "description": "for how many days ahead you wants the forecast"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          }
      },
      "required": ["location", "days"]
    }
  ],
  "stream": True,
  "function_call": "get_current_weather"
}

# Make your request and handle the response
sequences = llama.run(api_request_json)
for seq in sequences:
    print(seq)
```

Note: The run function is asynchronous and will return when all the data from the server has been received.

## Change Log
Version 0.1: Initial release

## Contributing
We welcome contributions to this project. Please see the Contributing Guidelines for more details.

## License
llamaapi SDK is licensed under the MIT License. Please see the License File for more details.
